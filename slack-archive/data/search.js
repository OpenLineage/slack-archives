window.search_data = {"channels":{"C01CK9T7HKR":"general","C01NAFMBVEY":"mark-grover","C030F1J0264":"dagster-integration","C04E3Q18RR9":"open-lineage-plus-bacalhau","C04JPTTC876":"spec-compliance","C04QSV0GG23":"providence-meetup","C04THH1V90X":"data-council-meetup","C051C93UZK9":"nyc-meetup","C055GGUFMHQ":"boston-meetup","C056YHEU680":"sf-meetup","C05N442RQUA":"toronto-meetup","C05PD7VJ52S":"london-meetup","C05U3UC85LM":"gx-integration","C065PQ4TL8K":"dev-discuss"},"users":{"U066S97A90C":"rwojcik","U066CNW85D3":"karthik.nandagiri","U066HKFCHUG":"naresh.naresh36","U05T8BJD4DU":"jasonyip","U02LXF3HUN7":"michael282","U05TU0U224A":"rodrigo.maia","U05NMJ0NBUK":"lance.dacey2","U05J9LZ355L":"yannick.libert.partne","U05JBHLPY8K":"athityakumar","U0635GK8Y14":"david.goss","U062Q95A1FG":"n.priya88","U063YP6UJJ0":"fangmik","U04AZ7992SU":"john490","U062WLFMRTP":"hloomba","U05CAULTYG2":"kkandaswamy","U06315TMT61":"splicer9904","U0625RZ7KR9":"kpraveen420","U05KCF3EEUR":"savansharan_navalgi","U03D8K119LJ":"matthewparas2020","U0616K9TSTZ":"ankit.goods10","U04EZ2LPDV4":"anirudh.shrinivason","U05HK41VCH1":"madhav.kakumani","U05QL7LN2GH":"jeevan","U021QJMRP47":"drew215","U01HVNU6A4C":"mars","U01DCLP0GU9":"julien","U05FLJE4GDU":"damien.hawes","U05TZE47F2S":"slack1950","U05A1D80QKF":"suraj.gupta","U05SMTVPPL3":"sangeeta","U05HFGKEYVB":"juan_luis_cano","U05SQGH8DV4":"sarathch","U05K8F1T887":"terese","U055N2GRT4P":"tatiana.alchueyr","U05QNRSQW1E":"sarwatfatimam","U0595Q78HUG":"gaborjbernat","U05HBLE7YPL":"abdallah","U0323HG8C8H":"sheeri.cabral","U05NGJ8AM8X":"yunhe52203334","U05EC8WB74N":"mbarrien","U05JY6MN8MS":"githubopenlineageissu","U05PVS8GRJ6":"josdotso","U05J5GRKY10":"george.polychronopoul","U05TQPZ4R4L":"aaruna6","U05Q3HT6PBR":"kevin","U05SXDWVA7K":"kgkwiz","U01HNKK4XAM":"harel.shein","U05QHG1NJ8J":"mike474","U01RA9B5GG2":"maciej.obuchowski","U0620HU51HA":"sicotte.jason","U05U9K21LSG":"bill","U02S6F54MAB":"jakub.dardzinski","U05U9929K3N":"don","U01DCMDFHBK":"willy","U02MK6YNAQ5":"pawel.leszczynski","U053LLVTHRN":"ross769","U05KKM07PJP":"peter.hicks"},"messages":{"C01CK9T7HKR":[{"m":"Hi Everyone, first of all - big shout to all contributors - You do amazing job here.\nI want to use OpenLineage in our project - to do so I want to setup some POC and experiment with possibilities library provides - I start working on sample from the conference talk: <https://github.com/getindata/openlineage-bbuzz2023-column-lineage> but when I go into spark transformation after staring context with openlineage I have issues with _SessionHiveMetaStoreClient on section 3_- does anyone has other plain sample to play with, to not setup everything from scratch?","u":"U066S97A90C","t":"1700568128.192669"},{"m":"Hi So we can use openlineage to identify column level lineage with Airflow , Spark? will it also allow to connect to Power BI and derive the downstream column lineage ?","u":"U066CNW85D3","t":"1700456258.614309"},{"m":"what are the pros and cons of OL. we often talk about positives to market it but what are the pain points using OL,how it's addressing user issues?","u":"U066HKFCHUG","t":"1700064658.956769"},{"m":"Can anyone tell me why OL is better than other competitors if you can provide an analysis that would be great","u":"U066HKFCHUG","t":"1700064564.825909"},{"m":"Hi\nCan anyone point me to the deck on how Airflow can be integrated using Openlineage?","u":"U066HKFCHUG","t":"1700050644.509419"},{"m":"<@U02MK6YNAQ5> I diff CreateReplaceDatasetBuilder.java and CreateReplaceOutputDatasetBuilder.java and they are the same except for the class name, so I am not sure what is causing the change. I also realize you don't have a test case for ADLS","u":"U05T8BJD4DU","t":"1699863517.394909"},{"m":"<@U02MK6YNAQ5> I went back to 1.4.1, output does show adls location. But environment facet is gone in 1.4.1. It shows up in 1.5.0 but namespace is back to dbfs....","u":"U05T8BJD4DU","t":"1699862442.876219"},{"m":"Databricks needs to be re-written in a way that supports Databricks it seems like","u":"U05T8BJD4DU","t":"1699691444.226989"},{"m":"<@U02MK6YNAQ5> this is why if create a table with adls location it won't show input and output:\n\n<https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/spark35/src/main/java/io/openlineage/spark35/agent/lifecycle/plan/CreateReplaceOutputDatasetBuilder.java#L146-L148|https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/spark35/src[…]k35/agent/lifecycle/plan/CreateReplaceOutputDatasetBuilder.java>\n\nBecause the catalog object is not there.","u":"U05T8BJD4DU","t":"1699691373.531469"},{"m":"<@U02MK6YNAQ5> regarding to <https://github.com/OpenLineage/OpenLineage/issues/2124>, OL is parsing out the table location in Hive metastore, it is the location of the table in the catalog and not the physical location of the data. It is both right and wrong because it is a table, just it is an external table.\n\n<https://docs.databricks.com/en/sql/language-manual/sql-ref-external-tables.html>","u":"U05T8BJD4DU","t":"1699647945.224489"},{"m":"<!channel>\nFriendly reminder: this month’s TSC meeting, open to all, is tomorrow at 10 am PT: <https://openlineage.slack.com/archives/C01CK9T7HKR/p1699027207361229>","u":"U02LXF3HUN7","t":"1699465494.687309"},{"m":"Has anyone here tried OpenLineage with Spark on Amazon EMR?","u":"U05TU0U224A","t":"1699465132.534889"},{"m":"if I have a dataset on adls gen2 which synapse connects to as an external delta table, is that the use case of a symlink dataset? the delta table is connected to by PBI and by Synapse, but the underlying data is exactly the same","u":"U05NMJ0NBUK","t":"1699372165.804069"},{"m":"Hi all, we (I work with <@U05VDHJJ9T7> and <@U05HBLE7YPL>) have a quick question regarding the spark integration:\nif a spark app contains several jobs, they will be named \"my_spark_app_name.job1\" and \"my_spark_app_name.job2\"\neg:\nspark_job.collect_limit\nspark_job.map_partitions_parallel_collection\n\nIf I understood correctly, the spark integration maps one Spark job to a single OpenLineage Job, and the application itself should be assigned a Run id at startup and each job that executes will report the application's Run id as its parent job run (taken from: <https://openlineage.io/docs/integrations/spark/>).\n\nIn our case, the app Run Id is never created, and the jobs runs don't contain any parent facets. We tested it with a recent integration version in 1.4.1 and also an older one (0.26.0).\nDid we miss something in the OL spark integration config?","u":"U05J9LZ355L","t":"1699355029.839029"},{"m":"Hey team! :wave:\n\nWe're trying to use openlineage-flink, and would like provide the `openlineage.transport.type=http`  and configure other transport configs, but we're not able to find sufficient docs (tried <https://openlineage.io/docs/integrations/flink|this doc>) on where/how these configs can be provided.\n\nFor example, in spark, the changes mostly were delegated to the spark-submit command like\n```spark-submit --conf \"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\" \\\n    --packages \"io.openlineage:openlineage-spark:&lt;spark-openlineage-version&gt;\" \\\n    --conf \"spark.openlineage.transport.url=http://{openlineage.client.host}/api/v1/namespaces/spark_integration/\" \\\n    --class com.mycompany.MySparkApp my_application.jar```\nAnd the `OpenLineageSparkListener` has a method to retrieve the provided spark confs as an object in the ArgumentParser. Similarly, looking for some pointers on how the openlineage.transport configs can be provided to `OpenLineageFlinkJobListener` &amp; how the flink listener parses/uses these configs\n\nTIA! :smile:","u":"U05JBHLPY8K","t":"1699266123.453379"},{"m":":wave: I raised a PR <https://github.com/OpenLineage/OpenLineage/pull/2223> off the back of some Marquez conversations a while back to try and clarify how names of Snowflake objects should be expressed in OL events. I used <https://github.com/Snowflake-Labs/OpenLineage-AccessHistory-Setup|Snowflake’s OL view> as a guide, but also I appreciate there are other OL producers that involve Snowflake too (Airflow? dbt?). Any feedback on this would be appreciated!","u":"U0635GK8Y14","t":"1699261422.618719"},{"m":"Hi Team , we are trying to customize the events by writing custom lineage listener extending OpenLineageSparkListener, but would need some direction how to capture the events","u":"U062Q95A1FG","t":"1699096090.087359"},{"m":"<!channel>\nThis month’s TSC meeting (open to all) is next Thursday the 9th at 10am PT. On the agenda:\n• announcements\n• recent releases\n• recent additions to the Flink integration by <@U05QA2D1XNV> \n• recent additions to the Spark integration by <@U02MK6YNAQ5> \n• updates on proposals by <@U01DCLP0GU9> \n• discussion topics\n• open discussion\nMore info and the meeting link can be found on the <https://openlineage.io/meetings/|website>. All are welcome! Do you have a discussion topic, use case or integration you’d like to demo? DM me to be added to the agenda.","u":"U02LXF3HUN7","t":"1699027207.361229"},{"m":"actually, it shows up in one of the RUNNING now... behavior is consistent between 11.3 and 13.3, thanks for fixing this issue","u":"U05T8BJD4DU","t":"1698999491.798599"},{"m":"<@U02MK6YNAQ5> I tested 1.5.0, it works great now, but the environment facets is gone in START... which I very much want it.. any thoughts?","u":"U05T8BJD4DU","t":"1698950958.157459"},{"m":"<!channel>\nWe released OpenLineage 1.5.0, including:\n• <https://github.com/OpenLineage/OpenLineage/pull/2175|support for Cassandra Connectors lineage in the Flink integration> by <@U05QA2D1XNV> \n• <https://github.com/OpenLineage/OpenLineage/pull/2185|support for Databricks Runtime 13.3 in the Spark integration> by <@U02MK6YNAQ5> \n• <https://github.com/OpenLineage/OpenLineage/pull/2188|support for >`rdd`<https://github.com/OpenLineage/OpenLineage/pull/2188| and >`toDF`<https://github.com/OpenLineage/OpenLineage/pull/2188| operations from the Spark Scala API in Spark> by <@U02MK6YNAQ5> \n• <https://github.com/OpenLineage/OpenLineage/pull/2107|lowered requirements for attrs and requests packages in the Airflow integration> by <@U02S6F54MAB> \n• <https://github.com/OpenLineage/OpenLineage/pull/2221|lazy rendering of yaml configs in the dbt integration> by <@U02S6F54MAB> \n• bug fixes, tests, infra fixes, doc changes, and more.\nThanks to all the contributors, including new contributor <@U05VDHJJ9T7>!\n*Release:* <https://github.com/OpenLineage/OpenLineage/releases/tag/1.5.0>\n*Changelog:* <https://github.com/OpenLineage/OpenLineage/blob/main/CHANGELOG.md>\n*Commit history:* <https://github.com/OpenLineage/OpenLineage/compare/1.4.1...1.5.0>\n*Maven:* <https://oss.sonatype.org/#nexus-search;quick~openlineage>\n*PyPI:* <https://pypi.org/project/openlineage-python/>","u":"U02LXF3HUN7","t":"1698940800.306129"},{"m":"I am looking to send OpenLineage events to an AWS API Gateway endpoint from an AWS MWAA instance. The problem is that all requests to AWS services need to be signed with SigV4, and using API Gateway with IAM authentication would require requests to API Gateway be signed with SigV4. Would the best way to do so be to just modify the python client HTTP transport to include a new config option for signing emitted OpenLineage events with SigV4? Are there any alternatives?","u":"U063YP6UJJ0","t":"1698885038.172079"},{"m":"Hi team :wave: , we’re finding that for our Spark jobs we are almost always getting some junk characters in our dataset names. We’ve pushed the regex filter to its limits and would like to extend the logic of deriving the dataset name in openlineage-spark (currently on `1.4.1`). I seem to recall hearing we could do this by implementing our own `LogicalPlanVisitor` or something along those lines? Is that still the recommended approach and if so would this be possible to implement in Scala vs. Java (scala noob here :simple_smile:)","u":"U04AZ7992SU","t":"1698882039.335099"},{"m":"<!channel>\nThe October 2023 issue of <https://mailchi.mp/cea829d27acd/openlineage-news-july-9597657?e=ef0563a7f8|OpenLineage News> is available now! <https://openlineage.us14.list-manage.com/track/click?u=fe7ef7a8dbb32933f30a10466&amp;id=123767f606&amp;e=ef0563a7f8|Sign up> to get in directly in your inbox each month.","u":"U02LXF3HUN7","t":"1698859749.531699"},{"m":"<!channel>\nI’m opening a vote to release OpenLineage 1.5.0, including:\n• support for Cassandra Connectors lineage in the Flink integration\n• support for Databricks Runtime 13.3 in the Spark integration\n• support for `rdd` and `toDF` operations from the Spark Scala API in Spark\n• lowered requirements for attrs and requests packages in the Airflow integration\n• lazy rendering of yaml configs in the dbt integration\n• bug fixes, tests, infra fixes, doc changes, and more.\nThree +1s from committers will authorize an immediate release.","u":"U02LXF3HUN7","t":"1698852883.658009"},{"m":"one question if someone is around - when im keeping both `openlineage-airflow` and `apache-airflow-providers-openlineage` in my requirement file,  i see the following error -\n```    from openlineage.airflow.extractors import Extractors\nModuleNotFoundError: No module named 'openlineage.airflow'```\nany thoughts?","u":"U062WLFMRTP","t":"1698778838.540239"},{"m":":wave: Hi team, cross-posting from the Marquez Channel in case anyone here has a better idea of the spec\n\n&gt; For most of our lineage extractors in airflow, we are using the rust sql parser from openlineage-sql to extract table lineage via sql statements. When errors occur we are adding an `extractionError` run facet similar to what is being done <https://github.com/OpenLineage/OpenLineage/blob/main/integration/airflow/openlineage/airflow/extractors/sql_extractor.py#L75-L89|here>. I’m finding in the case that multiple statements were extracted but one failed to parse while many others were successful, the lineage for these runs doesn’t appear as expected in Marquez. Is there any logic around the `extractionError` run facet that could be causing this? It seems reasonable to assume that we might take this to mean the entire run event is invalid if we have any extraction errors. \n&gt; \n&gt; I would still expect to see the other lineage we sent for the run but am instead just seeing the `extractionError` in the marquez UI, in the database, runs with an `extractionError` facet don’t seem to make it to the `job_versions_io_mapping` table","u":"U04AZ7992SU","t":"1698706303.956579"},{"m":"I realize in Spark 3.4+, some job ids don't have a start event. What part of the code is responsible for triggering the START and COMPLETE event","u":"U05T8BJD4DU","t":"1698563188.319939"},{"m":"Hello, has anyone run into similar error as posted in this github open issues[<https://github.com/MarquezProject/marquez/issues/2468>] while setting up marquez on an EC2 Instance, would appreciate any  help to get past the errors","u":"U05CAULTYG2","t":"1698440472.145489"},{"m":"referencing to <https://openlineage.slack.com/archives/C01CK9T7HKR/p1698398754823079?thread_ts=1698340358.557159&amp;cid=C01CK9T7HKR|this> conversation - what it takes to move to openlineage provider package from  openlineage-airflow. Im updating Airflow to 2.7.2 but moving off of openlineage-airflow to provider package Im trying to estimate the amount of work it takes, any thoughts? reading change_logs I dont think its too much of a change but please share your thoughts and if somewhere its drafted please do share that as well","u":"U062WLFMRTP","t":"1698429543.349989"},{"m":"Hi People, actually I want to intercept the OpenLineage spark events right after the job ends and before they are emitted, so that I can add some extra information to the events or remove some information that I don't want.\nIs there any way of doing this? Can someone please help me","u":"U06315TMT61","t":"1698408752.647169"},{"m":"*Spark Integration Logs*\nHey There\nAre these events skipped because it's not supported or it's configured somewhere?\n`23/10/27 08:25:58 INFO SparkSQLExecutionContext: OpenLineage received Spark event that is configured to be skipped: SparkListenerSQLExecutionStart`\n`23/10/27 08:25:58 INFO SparkSQLExecutionContext: OpenLineage received Spark event that is configured to be skipped: SparkListenerSQLExecutionEnd`","u":"U05TU0U224A","t":"1698400165.662489"},{"m":"Im upgrading the version from openlineage-airflow==0.24.0 to openlineage-airflow 1.4.1 but im seeing the following error, any help is appreciated","u":"U062WLFMRTP","t":"1698340358.557159"},{"m":"Hello Team","u":"U062WLFMRTP","t":"1698340277.847709"},{"m":"Hi I want to customise the events which comes from Openlineage spark . Can some one give some information","u":"U062Q95A1FG","t":"1698315220.142929"},{"m":"Hi,\n\nWe are using openlineage spark connector. We have used spark 3.2 and scala 2.12 so far. We have triggered a new job with Spark 3.4 and scala 2.13 and faced below exception.\n\n\n```java.lang.NoSuchMethodError: 'scala.collection.Seq org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.map(scala.Function1)'\n\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.lambda$buildInputDatasets$6(OpenLineageRunEventBuilder.java:341)\n\tat java.base/java.util.Optional.map(Optional.java:265)\n\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildInputDatasets(OpenLineageRunEventBuilder.java:339)\n\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.populateRun(OpenLineageRunEventBuilder.java:295)\n\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildRun(OpenLineageRunEventBuilder.java:279)\n\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildRun(OpenLineageRunEventBuilder.java:222)\n\tat io.openlineage.spark.agent.lifecycle.SparkSQLExecutionContext.start(SparkSQLExecutionContext.java:72)\n\tat io.openlineage.spark.agent.OpenLineageSparkListener.lambda$sparkSQLExecStart$0(OpenLineageSparkListener.java:91)```","u":"U0625RZ7KR9","t":"1697840317.080859"},{"m":"<@U05JY6MN8MS>\nI am trying to contribute to Integration tests which is listed here as <https://github.com/OpenLineage/OpenLineage/issues/2143|good first issue>\nthe <https://github.com/OpenLineage/OpenLineage/blob/main/CONTRIBUTING.md#triggering-ci-runs-from-forks-committers|CONTRIBUTING.md >mentions that i can trigger CI for integration tests from forked branch.\n<https://github.com/jklukas/git-push-fork-to-upstream-branch/blob/master/README.md#git-push-fork-to-upstream-branch|using this tool>.\nbut i am unable to do so, is there a way to trigger CI from forked brach or do i have to get permission from someone to run the CI?\n\ni am getting this error when i run this command `sudo git-push-fork-to-upstream-branch upstream savannavalgi:hacktober`\n&gt; ```Username for '<https://github.com>': savannavalgi\n&gt; Password for '<https://savannavalgi@github.com>': \n&gt; remote: Permission to OpenLineage/OpenLineage.git denied to savannavalgi.\n&gt; fatal: unable to access '<https://github.com/OpenLineage/OpenLineage.git/>': The requested URL returned error: 403```\ni have tried to configure ssh key\nalso tried to trigger CI from another brach,\nand tried all of this after fetching the latest upstream\n\ncc: <@U05JBHLPY8K> <@U01RA9B5GG2> <@U05HD9G5T17>","u":"U05KCF3EEUR","t":"1697805105.047909"},{"m":"Hey all - we've been noticing that some events go unreported by openlineage (spark) when the `AsyncEventQueue` fills up and starts dropping events. Wondering if anyone has experienced this before, and knows why it is happening? We've expanded the event queue capacity and thrown more hardware at the problem but no dice\n\nAlso as a note, the query plans from this job are pretty big - could the listener just be choking up? Happy to open a github issue as well if we suspect that it could be the listener itself having issues","u":"U03D8K119LJ","t":"1697742042.953399"},{"m":"Hello All, I am completely new for Openlineage, I have to setup the lab to conduct POC on various aspects like Lineage, metadata management , etc. As per openlineage site, i tried downloading Ubuntu, docker and binary files for Marquez. But I am lost somewhere and unable to configure whole setup. Can someone please assist in steps to start from scratch so that i can delve into the Openlineage capabilities. Many thanks","u":"U0616K9TSTZ","t":"1697597823.663129"},{"m":"Hello All :wave:!\nWe are currently trying to work the the *spark integration for OpenLineage in our Databricks instance*. The general setup is done and working with a few hicups here and there.\nBut one thing we are still struggling is how to link all spark jobs events with a Databricks job or a notebook run.\nWe´ve recently noticed that some of the events produced by OL have the \"environment-properties\" attribute with information (for our context) regarding notebook path (if it is a notebook run), or the the job run ID (if its a databricks job run). But the thing is that _these attributes are not always present._\nI ran some samples yesterday for a job with 4 notebook tasks. From all 20 json payload sent by the OL listener, only 3 presented the \"environment-properties\" attribute. Its not only happening with Databricks jobs. When i run single notebooks and each cell has its onw set of spark jobs, not all json events presented that property either.\n\nSo my question is *what is the criteria to have this attributes present or not in the event json file*? Or maybe this in an issue? <@U05T8BJD4DU> did you find out anything about this?\n\n:gear: Spark 3.4 / OL-Spark 1.4.1","u":"U05TU0U224A","t":"1697527077.180169"},{"m":"Hi team, I am running the following pyspark code in a cell:\n```print(\"SELECTING 100 RECORDS FROM METADATA TABLE\")\ndf = spark.sql(\"\"\"select * from <table> limit 100\"\"\")\n\nprint(\"WRITING (1) 100 RECORDS FROM METADATA TABLE\")\ndf.write.mode(\"overwrite\").format('delta').save(\"<s3 location 1>\")\ndf.createOrReplaceTempView(\"temp_metadata\")\n\nprint(\"WRITING (2) 100 RECORDS FROM METADATA TABLE\")\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"<s3 location 2>\")\n\nprint(\"READING (1) 100 RECORDS FROM METADATA TABLE\")\ndf_read = spark.read.format('delta').load(\"<s3 location 3>\")\ndf_read.createOrReplaceTempView(\"metadata_1\")\n\nprint(\"DOING THE MERGE INTO SQL STEP!\")\ndf_new = spark.sql(\"\"\"\n    MERGE INTO metadata_1\n    USING <table>\n    ON metadata_1.id = temp_metadata.id\n    WHEN MATCHED THEN UPDATE SET \n        metadata_1.id = temp_metadata.id,\n        metadata_1.aspect = temp_metadata.aspect\n    WHEN NOT MATCHED THEN INSERT (id, aspect) \n        VALUES (temp_metadata.id, temp_metadata.aspect)\n\"\"\")```\nI am running with debug log levels. I actually don't see any of the events being logged for `SaveIntoDataSourceCommand` or the `MergeIntoCommand`, but OL is in fact emitting events to the backend. It seems like the events are just not being logged... I actually observe this for all delta table related spark sql queries...","u":"U04EZ2LPDV4","t":"1697179720.032079"},{"m":"This might be a dumb question, I guess I need to setup local Spark in order for the Spark tests to run successfully?","u":"U05T8BJD4DU","t":"1697137714.503349"},{"m":"<!channel>\nFriendly reminder: this month’s TSC meeting, open to all, is tomorrow at 10 am PT: <https://openlineage.slack.com/archives/C01CK9T7HKR/p1696531454431629>","u":"U02LXF3HUN7","t":"1697043601.182719"},{"m":"Hi @there, I am trying to make API call to get column-lineage information could you please let me know the url construct to retrieve the same? As per the API documentation I am passing the following url to GET column-lineage: <http://localhost:5000/api/v1/column-lineage> but getting error code:400. Thanks","u":"U05HK41VCH1","t":"1697040264.029839"},{"m":"<!here> When i am running this sql as part of a databricks notebook, i am recieving an OL event where i see only an output dataset and there is no input dataset or  a symlink facet inside the dataset to map it to the underlying azure storage object. Can anyone kindly help on this\n```spark.sql(f\"CREATE TABLE IF NOT EXISTS covid_research.uscoviddata USING delta LOCATION '<abfss://oltptestdata@jeevanacceldata.dfs.core.windows.net/testdata/modified-delta>'\")\n{\n    \"eventTime\": \"2023-10-11T10:47:36.296Z\",\n    \"producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n    \"schemaURL\": \"<https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent>\",\n    \"eventType\": \"COMPLETE\",\n    \"run\": {\n        \"runId\": \"d0f40be9-b921-4c84-ac9f-f14a86c29ff7\",\n        \"facets\": {\n            \"spark.logicalPlan\": {\n                \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                \"_schemaURL\": \"<https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet>\",\n                \"plan\": [\n                    {\n                        \"class\": \"org.apache.spark.sql.catalyst.plans.logical.CreateTable\",\n                        \"num-children\": 1,\n                        \"name\": 0,\n                        \"tableSchema\": [],\n                        \"partitioning\": [],\n                        \"tableSpec\": null,\n                        \"ignoreIfExists\": true\n                    },\n                    {\n                        \"class\": \"org.apache.spark.sql.catalyst.analysis.ResolvedIdentifier\",\n                        \"num-children\": 0,\n                        \"catalog\": null,\n                        \"identifier\": null\n                    }\n                ]\n            },\n            \"spark_version\": {\n                \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                \"_schemaURL\": \"<https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet>\",\n                \"spark-version\": \"3.3.0\",\n                \"openlineage-spark-version\": \"1.2.2\"\n            },\n            \"processing_engine\": {\n                \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-1-0/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet>\",\n                \"version\": \"3.3.0\",\n                \"name\": \"spark\",\n                \"openlineageAdapterVersion\": \"1.2.2\"\n            }\n        }\n    },\n    \"job\": {\n        \"namespace\": \"default\",\n        \"name\": \"adb-3942203504488904.4.azuredatabricks.net.create_table.covid_research_db_uscoviddata\",\n        \"facets\": {}\n    },\n    \"inputs\": [],\n    \"outputs\": [\n        {\n            \"namespace\": \"dbfs\",\n            \"name\": \"/user/hive/warehouse/covid_research.db/uscoviddata\",\n            \"facets\": {\n                \"dataSource\": {\n                    \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                    \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-0-0/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet>\",\n                    \"name\": \"dbfs\",\n                    \"uri\": \"dbfs\"\n                },\n                \"schema\": {\n                    \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                    \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-0-0/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet>\",\n                    \"fields\": []\n                },\n                \"storage\": {\n                    \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                    \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-0-0/StorageDatasetFacet.json#/$defs/StorageDatasetFacet>\",\n                    \"storageLayer\": \"unity\",\n                    \"fileFormat\": \"parquet\"\n                },\n                \"symlinks\": {\n                    \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                    \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-0-0/SymlinksDatasetFacet.json#/$defs/SymlinksDatasetFacet>\",\n                    \"identifiers\": [\n                        {\n                            \"namespace\": \"/user/hive/warehouse/covid_research.db\",\n                            \"name\": \"covid_research.uscoviddata\",\n                            \"type\": \"TABLE\"\n                        }\n                    ]\n                },\n                \"lifecycleStateChange\": {\n                    \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.2.2/integration/spark>\",\n                    \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-0-0/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet>\",\n                    \"lifecycleStateChange\": \"CREATE\"\n                }\n            },\n            \"outputFacets\": {}\n        }\n    ]\n}```","u":"U05QL7LN2GH","t":"1697021758.073929"},{"m":"<!here> i am trying out the databricks spark integration and in one of the events i am getting a openlineage event where the output dataset is having a facet called `symlinks` , the statement that generated this event is this sql\n```CREATE TABLE IF NOT EXISTS covid_research.covid_data \nUSING CSV\nLOCATION '<abfss://oltptestdata@jeevanacceldata.dfs.core.windows.net/testdata/johns-hopkins-covid-19-daily-dashboard-cases-by-states.csv>' \nOPTIONS (header \"true\", inferSchema \"true\");```\nCan someone kindly let me know what this `symlinks` facet is. i tried seeing the spec but did not get it completely","u":"U05QL7LN2GH","t":"1696995819.546399"},{"m":"example:\n\n```{\"environment-properties\":{\"spark.databricks.clusterUsageTags.clusterName\":\"<mailto:jason.yip@tredence.com|jason.yip@tredence.com>'s Cluster\",\"spark.databricks.job.runId\":\"\",\"spark.databricks.job.type\":\"\",\"spark.databricks.clusterUsageTags.azureSubscriptionId\":\"a4f54399-8db8-4849-adcc-a42aed1fb97f\",\"spark.databricks.notebook.path\":\"/Repos/jason.yip@tredence.com/segmentation/01_Data Prep\",\"spark.databricks.clusterUsageTags.clusterOwnerOrgId\":\"4679476628690204\",\"MountPoints\":[{\"MountPoint\":\"/databricks-datasets\",\"Source\":\"databricks-datasets\"},{\"MountPoint\":\"/Volumes\",\"Source\":\"UnityCatalogVolumes\"},{\"MountPoint\":\"/databricks/mlflow-tracking\",\"Source\":\"databricks/mlflow-tracking\"},{\"MountPoint\":\"/databricks-results\",\"Source\":\"databricks-results\"},{\"MountPoint\":\"/databricks/mlflow-registry\",\"Source\":\"databricks/mlflow-registry\"},{\"MountPoint\":\"/Volume\",\"Source\":\"DbfsReserved\"},{\"MountPoint\":\"/volumes\",\"Source\":\"DbfsReserved\"},{\"MountPoint\":\"/\",\"Source\":\"DatabricksRoot\"},{\"MountPoint\":\"/volume\",\"Source\":\"DbfsReserved\"}],\"User\":\"<mailto:jason.yip@tredence.com|jason.yip@tredence.com>\",\"UserId\":\"4768657035718622\",\"OrgId\":\"4679476628690204\"}}```","u":"U05T8BJD4DU","t":"1696985639.868119"},{"m":"Any idea why \"environment-properties\" is gone in Spark 3.4+ in StartEvent?","u":"U05T8BJD4DU","t":"1696914311.793789"},{"m":"Hello.  I am getting started with OL and Marquez with dbt.  I am using dbt-ol.  The namespace of the dataset showing up in Marquez is not the namespace I provide using OPENLINEAGE_NAMESPACE.  It happens to be the same as the source in Marquez which is the snowflake account uri.  It's obviously picking up the other env variable OPENLINEAGE_URL so i am pretty sure its not the environment.  Is this expected?","u":"U021QJMRP47","t":"1696884935.692409"},{"m":"<!channel>\n*We released OpenLineage 1.4.1!*\n*Additions:*\n• *Client:* *allow setting client’s endpoint via environment variable* <http://2151.com/OpenLineage/OpenLineage/pull/2151|2151> <@U01HVNU6A4C> \n• *Flink: expand Iceberg source types* <https://github.com/OpenLineage/OpenLineage/pull/2149|2149> <@U05QA2D1XNV> \n• *Spark: add debug facet* <https://github.com/OpenLineage/OpenLineage/pull/2147|2147> <@U02MK6YNAQ5> \n• *Spark: enable Nessie REST catalog* <https://github.com/OpenLineage/OpenLineage/pull/2165|2165> <https://github.com/julwin|@julwin> \nThanks to all the contributors, especially new contributors <@U05QA2D1XNV> and <https://github.com/julwin|@julwin>!\n*Release:* <https://github.com/OpenLineage/OpenLineage/releases/tag/1.4.1>\n*Changelog:* <https://github.com/OpenLineage/OpenLineage/blob/main/CHANGELOG.md>\n*Commit history:* <https://github.com/OpenLineage/OpenLineage/compare/1.3.1...1.4.1>\n*Maven:* <https://oss.sonatype.org/#nexus-search;quick~openlineage>\n*PyPI:* <https://pypi.org/project/openlineage-python/>","u":"U02LXF3HUN7","t":"1696879514.895109"},{"m":"<!here> I am trying out the openlineage integration of spark on databricks. There is no event getting emitted from Openlineage, I see logs saying OpenLineage Event Skipped. I am attaching the Notebook that i am trying to run and the cluster logs. Kindly can someone help me on this","u":"U05QL7LN2GH","t":"1696823976.297949"},{"m":"<@U02LXF3HUN7> can we cut a new release to include this change?\n• <https://github.com/OpenLineage/OpenLineage/pull/2151>","u":"U01HVNU6A4C","t":"1696591141.778179"},{"m":"The Marquez meetup in San Francisco is happening right now!\n<https://www.meetup.com/meetup-group-bnfqymxe/events/295444209/?utm_medium=referral&utm_campaign=share-btn_savedevents_share_modal&utm_source=link|https://www.meetup.com/meetup-group-bnfqymxe/events/295444209/?utm_medium=referral&utm_campaign=share-btn_savedevents_share_modal&utm_source=link>","u":"U01DCLP0GU9","t":"1696552840.350759"},{"m":"I have created a ticket to make this easier to find. Once I get more feedback I’ll turn it into a md file in the repo: <https://docs.google.com/document/d/1zIxKST59q3I6ws896M4GkUn7IsueLw8ejct5E-TR0vY/edit#heading=h.enpbmvu7n8gu>\n<https://github.com/OpenLineage/OpenLineage/issues/2161>","u":"U01DCLP0GU9","t":"1696541652.452819"},{"m":"*<!channel>*\nThis month’s TSC meeting is next Thursday the 12th at 10am PT. On the tentative agenda:\n• announcements\n• recent releases\n• Airflow Summit recap\n• tutorial: migrating to the Airflow Provider\n• discussion topic: observability for OpenLineage/Marquez\n• open discussion\n• more (TBA)\nMore info and the meeting link can be found on the <https://openlineage.io/meetings/|website>. All are welcome! Do you have a discussion topic, use case or integration you’d like to demo? DM me to be added to the agenda.","u":"U02LXF3HUN7","t":"1696531454.431629"},{"m":"I have cleaned up the registry proposal.\n<https://docs.google.com/document/d/1zIxKST59q3I6ws896M4GkUn7IsueLw8ejct5E-TR0vY/edit>\nIn particular:\n• I clarified that option 2 is preferred at this point.\n• I moved discussion notes to the bottom. they will go away at some point\n• Once it is stable, I’ll create a <https://github.com/OpenLineage/OpenLineage/tree/main/proposals|proposal> with the preferred option.\n• we need a good proposal for the core facets prefix. My suggestion is to move core facets to `core` in the registry. The drawback is prefix would be inconsistent.\n","u":"U01DCLP0GU9","t":"1696379615.265919"},{"m":"Hey everyone - does anyone have a good mechanism for alerting on issues with open lineage? For example, maybe alerting when an event times out - perhaps to prometheus or some other kind of generic endpoint? Not sure the best approach here (if the meta inf extension would be able to achieve it)","u":"U03D8K119LJ","t":"1696350897.139129"},{"m":"<!channel>\n*We released OpenLineage 1.3.1!*\n*Added:*\n• Airflow: add some basic stats to the Airflow integration `#1845` <https://github.com/harels|@harels>\n• Airflow: add columns as schema facet for `airflow.lineage.Table` (if defined) `#2138` <https://github.com/erikalfthan|@erikalfthan>\n• DBT: add SQLSERVER to supported dbt profile types `#2136` <https://github.com/erikalfthan|@erikalfthan>\n• Spark: support for latest 3.5 `#2118` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\n*Fixed:*\n• Airflow: fix find-links path in tox `#2139` <https://github.com/JDarDagran|@JDarDagran>\n• Airflow: add more graceful logging when no OpenLineage provider installed `#2141` <https://github.com/JDarDagran|@JDarDagran>\n• Spark: fix bug in PathUtils’ `prepareDatasetIdentifierFromDefaultTablePath` (CatalogTable) to correctly preserve scheme from `CatalogTable`’s location `#2142` <https://github.com/d-m-h|@d-m-h>\nThanks to all the contributors, including new contributor <@U05TZE47F2S>!\n*Release:* <https://github.com/OpenLineage/OpenLineage/releases/tag/1.3.1>\n*Changelog:* <https://github.com/OpenLineage/OpenLineage/blob/main/CHANGELOG.md>\n*Commit history:* <https://github.com/OpenLineage/OpenLineage/compare/1.2.2...1.3.1>\n*Maven:* <https://oss.sonatype.org/#nexus-search;quick~openlineage>\n*PyPI:* <https://pypi.org/project/openlineage-python/>","u":"U02LXF3HUN7","t":"1696344963.496819"},{"m":"Hi folks - I'm wondering if its just me, but does `io.openlineage:openlineage-sql-java:1.2.2` ship with the `arm64.dylib` binary? When i try and run code that uses the Java package on an Apple M1, the binary isn't found, The workaround is to checkout 1.2.2 and then build and publish it locally.","u":"U05FLJE4GDU","t":"1696319076.770719"},{"m":"<!channel>\nThe September issue of <https://mailchi.mp/3b9bbb3eba23/openlineage-news-july-9591485?e=ce16eef4ef|OpenLineage News> is here! This issue covers the big news about OpenLineage coming out of Airflow Summit, progress on the Airflow Provider, highlights from our meetup in Toronto, and much more.\nTo get the newsletter directly in your inbox each month, sign up <http://bit.ly/OL_news|here>.","u":"U02LXF3HUN7","t":"1696264108.497989"},{"m":"<!channel>\nHello all, I’d like to open a vote to release OpenLineage 1.3.0, including:\n• support for Spark 3.5 in the Spark integration\n• scheme preservation bug fix in the Spark integration\n• find-links path in tox bug in the Airflow integration fix\n• more graceful logging when no OL provider is installed in the Airflow integration\n• columns as schema facet for airflow.lineage.Table addition\n• SQLSERVER to supported dbt profile types addition\nThree +1s from committers will authorize. Thanks in advance.","u":"U02LXF3HUN7","t":"1696262312.791719"},{"m":"Are you located in the Brussels area or within commutable distance? Interested in attending a meetup between October 16-20? If so, please DM <@U0323HG8C8H> or myself. TIA","u":"U02LXF3HUN7","t":"1695932184.205159"},{"m":"Hello community\nFirst time poster - bear with me :)\n\nI am looking to make a minor PR on the airflow integration (fixing github #2130), and the code change is easy enough, but I fail to install the python environment. I have tried the simple ones\n`OpenLineage/integration/airflow &gt; pip install -e .`\n or\n`OpenLineage/integration/airflow &gt; pip install -r dev-requirements.txt`\nbut they both fail on\n`ERROR: No matching distribution found for openlineage-sql==1.3.0`\n\n(which I think is an unreleased version in the git project)\n\nHow would I go about to install the requirements?\n\n//Erik\n\nPS. Sorry for posting this in general if there is a specific integration or contribution channel - I didnt find a better channel","u":"U05TZE47F2S","t":"1695883240.832669"},{"m":"Hi folks, am I correct in my observations that the Spark integration does not generate inputs and outputs for Kafka-to-Kafka pipelines?\n\n*EDIT:* Removed the crazy wall of text. Relevant GitHub issue is <https://github.com/OpenLineage/OpenLineage/issues/2137|here>.","u":"U05FLJE4GDU","t":"1695831785.042079"},{"m":"*Meetup recap: Toronto Meetup @ Airflow Summit, September 18, 2023*\nIt was great to see so many members of our community at this event! I counted 32 total attendees, with all but a handful being first-timers.\n*Topics included:*\n• Presentation on the history, architecture and roadmap of the project by <@U01DCLP0GU9> and <@U01HNKK4XAM> \n• Discussion of OpenLineage support in <https://marquezproject.ai/|Marquez> by <@U01DCMDFHBK> \n• Presentation by *Ye Liu* and *Ivan Perepelitca* from <https://metaphor.io/|Metaphor>, the social platform for data, about their integration\n• Presentation by <@U02MK6YNAQ5> about the Spark integration\n• Presentation by <@U01RA9B5GG2> about the Apache Airflow Provider\nThanks to all the presenters and attendees with a shout out to <@U01HNKK4XAM> for the help with organizing and day-of logistics, <@U02S6F54MAB> for the help with set up/clean up, and <@U0323HG8C8H> for the crucial assist with the signup sheet.\nThis was our first meetup in Toronto, and we learned some valuable lessons about planning events in new cities — the first and foremost being to ask for a pic of the building! :slightly_smiling_face: But it seemed like folks were undeterred, and the space itself lived up to expectations.\nFor a recording and clips from the meetup, head over to our <https://www.youtube.com/channel/UCRMLy4AaSw_ka-gNV9nl7VQ/|YouTube channel>.\n*Upcoming events:*\n• October 5th in San Francisco: Marquez Meetup @ Astronomer (sign up <https://www.meetup.com/meetup-group-bnfqymxe/events/295444209/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link|here>)\n• November: Warsaw meetup (details, date TBA)\n• January: London meetup (details, date TBA)\nAre you interested in hosting or co-hosting an OpenLineage or Marquez meetup? DM me!","u":"U02LXF3HUN7","t":"1695827956.140429"},{"m":"<!here> In Airflow Integration we send across a lineage Event for Dag start and complete, but that is not the case with spark integration…we don’t receive any event for the application start and complete in spark…is this expected behaviour or am i missing something?","u":"U05QL7LN2GH","t":"1695703890.171789"},{"m":"I'm using the Spark OpenLineage integration. In the `outputStatistics` output dataset facet we receive `rowCount` and `size`.\nThe Job performs a SQL insert into a MySQL table and I'm receiving the `size` as 0.\n```{\n  \"outputStatistics\":\n  {\n    \"_producer\": \"<https://github.com/OpenLineage/OpenLineage/tree/1.1.0/integration/spark>\",\n    \"_schemaURL\": \"<https://openlineage.io/spec/facets/1-0-0/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet>\",\n    \"rowCount\": 1,\n    \"size\": 0\n   }\n}```\nI'm not sure what the size means here. Does this mean number of bytes inserted/updated?\nAlso, do we have any documentation for Spark specific Job and Run facets?","u":"U05A1D80QKF","t":"1695663385.834539"},{"m":"<!here> I'm presently addressing a particular scenario that pertains to Openlineage authentication, specifically involving the use of an access key and secret.\n\nI've implemented a custom token provider called AccessKeySecretKeyTokenProvider, which extends the TokenProvider class. This token provider communicates with another service, obtaining a token and an expiration time based on the provided access key, secret, and client ID.\n\nMy goal is to retain this token in a cache prior to its expiration, thereby eliminating the need for network calls to the third-party service. Is it possible without relying on an external caching system.","u":"U05SMTVPPL3","t":"1695633110.066819"},{"m":"I am attaching the log4j, there is no openlineagecontext","u":"U05T8BJD4DU","t":"1695352570.560639"},{"m":"I installed 1.2.2 on Databricks, followed the below init script: <https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/databricks/open-lineage-init-script.sh>\n\nmy cluster config looks like this:\n\nspark.openlineage.version v1\nspark.openlineage.namespace adb-5445974573286168.8#default\nspark.openlineage.endpoint v1/lineage\nspark.openlineage.url.param.code 8kZl0bo2TJfnbpFxBv-R2v7xBDj-PgWMol3yUm5iP1vaAzFu9kIZGg==\nspark.openlineage.url <https://f77b-50-35-69-138.ngrok-free.app>\n\nBut it is not calling the API, it works fine with 0.18 version","u":"U05T8BJD4DU","t":"1695347501.889769"},{"m":"I am using this accelerator that leverages OpenLineage on Databricks to publish lineage info to Purview, but it's using a rather old version of OpenLineage aka 0.18, anybody has tried it on a newer version of OpenLineage? I am facing some issues with the inputs and outputs for the same object is having different json\n<https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator/|https://github.com/microsoft/Purview-ADB-Lineage-Solution-Accelerator/>","u":"U05T8BJD4DU","t":"1695335777.852519"},{"m":"Hi, we're using Custom Operators in airflow(2.5) and are planning to expose lineage via default extractors: <https://openlineage.io/docs/integrations/airflow/default-extractors/>\n*Question*: Now if we upgrade our Airflow version to 2.7 in the future, would our code be backward compatible?\nSince OpenLineage has now moved inside airflow and I think there is no concept of extractors in the latest version.","u":"U05A1D80QKF","t":"1695276670.439269"},{"m":"<!channel>\nWe released OpenLineage 1.2.2!\nAdded\n• Spark: publish the `ProcessingEngineRunFacet` as part of the normal operation of the `OpenLineageSparkEventListener` `#2089` <https://github.com/d-m-h|@d-m-h>\n• Spark: capture and emit `spark.databricks.clusterUsageTags.clusterAllTags` variable from databricks environment `#2099` <https://github.com/Anirudh181001|@Anirudh181001>\nFixed\n• Common: support parsing dbt_project.yml without target-path `#2106` <https://github.com/tatiana|@tatiana>\n• Proxy: fix Proxy chart `#2091` <https://github.com/harels|@harels>\n• Python: fix serde filtering `#2044` <https://github.com/xli-1026|@xli-1026>\n• Python: use non-deprecated `apiKey` if loading it from env variables `@2029` <https://github.com/mobuchowski|@mobuchowski>\n• Spark: Improve RDDs on S3 integration. `#2039` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\n• Flink: prevent sending `running` events after job completes `#2075` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\n• Spark &amp; Flink: Unify dataset naming from URI objects `#2083` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\n• Spark: Databricks improvements `#2076` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\nRemoved\n• SQL: remove sqlparser dependency from iface-java and iface-py `#2090` <https://github.com/JDarDagran|@JDarDagran>\nThanks to all the contributors, including new contributors <@U055N2GRT4P>, <https://github.com/xli-1026|@xli-1026>, and <https://github.com/d-m-h|@d-m-h>!\n*Release:* <https://github.com/OpenLineage/OpenLineage/releases/tag/1.2.2>\n*Changelog:* <https://github.com/OpenLineage/OpenLineage/blob/main/CHANGELOG.md>\n*Commit history:* <https://github.com/OpenLineage/OpenLineage/compare/1.1.0...1.2.0|https://github.com/OpenLineage/OpenLineage/compare/1.1.0...1.2.2>\n*Maven:* <https://oss.sonatype.org/#nexus-search;quick~openlineage>\n*PyPI:* <https://pypi.org/project/openlineage-python/>","u":"U02LXF3HUN7","t":"1695244138.650089"},{"m":"congrats folks :partying_face: <https://lfaidata.foundation/blog/2023/09/20/lf-ai-data-foundation-announces-graduation-of-openlineage-project>","u":"U05HFGKEYVB","t":"1695217014.549799"},{"m":"Hi I need help in extracting OpenLineage for PostgresOperator in json format.\nany suggestions or comments would be greatly appreciated","u":"U05SQGH8DV4","t":"1695106067.665469"},{"m":"Hi! I'm in need of help with wrapping my head around OpenLineage. My team have the goal of collecting metadata from the Airflow operators GreatExpectationsOperator, PythonOperator, MsSqlOperator and BashOperator (for dbt). Where can I see the sourcecode for what is collected for each operator, and is there support for these in the new provider *apache-airflow-providers-openlineage*? I am super confused and feel lost in the docs. :exploding_head: We are using MSSQL/ODBC to connect to our db, and this data does not seem to appear as datasets in Marquez, do I need to configure this? If so, HOW and WHERE? :smiling_face_with_tear:\n\nHappy for any help, big or small! :pray:","u":"U05K8F1T887","t":"1695039754.591479"},{"m":"It doesn't seem like there's a way to override the OL endpoint from the default (`/api/v1/lineage`) in Airflow? I tried setting the `OPENLINEAGE_ENDPOINT` environment to no avail. Based on <https://github.com/OpenLineage/OpenLineage/blob/main/client/python/openlineage/client/transport/factory.py#L80-L87|this statement>, it seems that only `OPENLINEAGE_URL` was used to construct `HttpConfig` ?","u":"U01HVNU6A4C","t":"1694956061.909169"},{"m":"<!here> is there a way by which we could add custom headers to openlineage client in airflow, i see that provision is there for spark integration via these properties <http://spark.openlineage.transport.headers.xyz|spark.openlineage.transport.headers.xyz> --&gt; abcdef","u":"U05QL7LN2GH","t":"1694907627.974239"},{"m":"<!here> we have dataproc operator getting called from a dag which submits a spark job, we wanted to maintain that continuity of parent job in the spark job so according to the documentation we can acheive that by using a macro called lineage_run_id that requires task and task_instance as the parameters. The problem we are facing is that our client’s have 1000's of dags, so asking them to change this everywhere it is used is not feasible, so we thought of using the task_policy feature in the airflow…but the problem is that task_policy gives you access to only the task/operator, but we don’t have the access to the task instance..that is required as a parameter to the lineage_run_id function. Can anyone kindly help us on how should we go about this one\n```t1 = DataProcPySparkOperator(\n    task_id=job_name,\n    #required pyspark configuration,\n    job_name=job_name,\n    dataproc_pyspark_properties={\n        'spark.driver.extraJavaOptions':\n            f\"-javaagent:{jar}={os.environ.get('OPENLINEAGE_URL')}/api/v1/namespaces/{os.getenv('OPENLINEAGE_NAMESPACE', 'default')}/jobs/{job_name}/runs/{{{{macros.OpenLineagePlugin.lineage_run_id(task, task_instance)}}}}?api_key={os.environ.get('OPENLINEAGE_API_KEY')}\"\n        dag=dag)```","u":"U05QL7LN2GH","t":"1694849427.228709"},{"m":"<!channel>\nFriendly reminder: the next OpenLineage meetup, our first in Toronto, is happening this coming Monday at 5 PM ET <https://openlineage.slack.com/archives/C01CK9T7HKR/p1694441261486759>","u":"U02LXF3HUN7","t":"1694793807.376729"},{"m":"Per discussion in the OpenLineage sync today here is a very early strawman proposal for an OpenLineage registry that producers and consumers could be registered in.\nFeedback or alternate proposals welcome\n<https://docs.google.com/document/d/1zIxKST59q3I6ws896M4GkUn7IsueLw8ejct5E-TR0vY/edit>\nOnce this is sufficiently fleshed out, I’ll create an actual proposal on github","u":"U01DCLP0GU9","t":"1694737381.437569"},{"m":"Hey everyone,\nAny chance we could have a *openlineage-integration-common* 1.1.1 release with the following changes..?\n• <https://github.com/OpenLineage/OpenLineage/pull/2106>\n• <https://github.com/OpenLineage/OpenLineage/pull/2108>","u":"U055N2GRT4P","t":"1694700221.242579"},{"m":"Context:\n\nWe use Spark with YARN, running on Hadoop 2.x (I can't remember the exact minor version) with Hive support.\n\nProblem:\n\nI'm noticed that `CreateDataSourceAsSelectCommand` objects are _always_ transformed to an `OutputDataset` with a _namespace_ value set to `file` - which is curious, because the inputs always have a (correct) namespace of `hdfs://&lt;name-node&gt;` - is this a known issue? A flaw with Apache Spark? A bug in the resolution logic?\n\nFor reference:\n\n```public class CreateDataSourceTableCommandVisitor\n    extends QueryPlanVisitor&lt;CreateDataSourceTableCommand, OpenLineage.OutputDataset&gt; {\n\n  public CreateDataSourceTableCommandVisitor(OpenLineageContext context) {\n    super(context);\n  }\n\n  @Override\n  public List&lt;OpenLineage.OutputDataset&gt; apply(LogicalPlan x) {\n    CreateDataSourceTableCommand command = (CreateDataSourceTableCommand) x;\n    CatalogTable catalogTable = command.table();\n\n    return Collections.singletonList(\n        outputDataset()\n            .getDataset(\n                PathUtils.fromCatalogTable(catalogTable),\n                catalogTable.schema(),\n                OpenLineage.LifecycleStateChangeDatasetFacet.LifecycleStateChange.CREATE));\n  }\n}```\nRunning this: `cat events.log | jq '{eventTime: .eventTime, eventType: .eventType, runId: .run.runId, jobNamespace: .job.namespace, jobName: .job.name, outputs: .outputs[] | {namespace: .namespace, name: .name}, inputs: .inputs[] | {namespace: .namespace, name: .name}}'`\n\nThis is an output:\n```{\n  \"eventTime\": \"2023-09-13T16:01:27.059Z\",\n  \"eventType\": \"START\",\n  \"runId\": \"bbbb5763-3615-46c0-95ca-1fc398c91d5d\",\n  \"jobNamespace\": \"spark.cluster-1\",\n  \"jobName\": \"ol_hadoop_test.execute_create_data_source_table_as_select_command.dhawes_db_ol_test_hadoop_tgt\",\n  \"outputs\": {\n    \"namespace\": \"file\",\n    \"name\": \"/user/hive/warehouse/dhawes.db/ol_test_hadoop_tgt\"\n  },\n  \"inputs\": {\n    \"namespace\": \"<hdfs://nn1>\",\n    \"name\": \"/user/hive/warehouse/dhawes.db/ol_test_hadoop_src\"\n  }\n}```","u":"U05FLJE4GDU","t":"1694686815.337029"},{"m":"<!channel>\nThis month’s TSC meeting, open to all, is tomorrow: <https://openlineage.slack.com/archives/C01CK9T7HKR/p1694113940400549>","u":"U02LXF3HUN7","t":"1694629232.934029"},{"m":"I am exploring Spark - OpenLineage integration (using the latest PySpark and OL versions). I tested a simple pipeline which:\n• Reads JSON data into PySpark DataFrame\n• Apply data transformations\n• Write transformed data to MySQL database\nObserved that we receive 4 events (2 `START` and 2 `COMPLETE`) for the same job name. The events are almost identical with a small diff in the facets. All the events share the same `runId`, and we don't get any `parentRunId`.\nTeam, can you please confirm if this behaviour is expected? Seems to be different from the Airflow integration where we relate jobs to Parent Jobs.","u":"U05A1D80QKF","t":"1694583867.900909"},{"m":"<!here> has anyone succeded in getting a custom extractor to work in GCP Cloud Composer or AWS MWAA, seems like there is no way","u":"U05QL7LN2GH","t":"1694553961.188719"},{"m":"I am trying to run Google Cloud Composer where i have added the openlineage-airflow pypi packagae as a dependency and have added the env OPENLINEAGE_EXTRACTORS to point to my custom extractor. I have added a folder by name dependencies and inside that i have placed my extractor file, and the path given to  OPENLINEAGE_EXTRACTORS is dependencies.&lt;file_name&gt;.&lt;extractor_class_name&gt;…still it fails with the exception saying    No module named ‘dependencies’. Can anyone kindly help me out on correcting my mistake","u":"U05QL7LN2GH","t":"1694545905.974339"},{"m":"This particular code in docker-compose exits with code 1 because it is unable to find wait-for-it.sh, file in the container. I have checked the mounting path from the local machine, It is correct and the path on the container for Marquez is also correct i.e. /usr/src/app but it is unable to mount the wait-for-it.sh. Does anyone know why is this? This code exists in the open lineage repository as well <https://github.com/OpenLineage/OpenLineage/blob/main/integration/spark/docker-compose.yml>\n```# Marquez as an OpenLineage Client\n  api:\n    image: marquezproject/marquez\n    container_name: marquez-api\n    ports:\n      - \"5000:5000\"\n      - \"5001:5001\"\n    volumes:\n      - ./docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh\n    links:\n      - \"db:postgres\"\n    depends_on:\n      - db\n    entrypoint: [ \"./wait-for-it.sh\", \"db:5432\", \"--\", \"./entrypoint.sh\" ]```","u":"U05QNRSQW1E","t":"1694520846.519609"},{"m":"Opened a PR for this here: <https://github.com/OpenLineage/OpenLineage/pull/2100>","u":"U04AZ7992SU","t":"1694468262.274069"},{"m":"I’m seeing some odd behavior with my http transport when upgrading airflow/openlineage-airflow from 2.3.2 -&gt; 2.6.3 and 0.24.0 -&gt; 0.28.0. Previously I had a config like this that let me provide my own auth tokens. However, after upgrading I’m getting a 401 from the endpoint and further debugging seems to reveal that we’re not using the token provided in my TokenProvider. Does anyone know if something changed between these versions that could be causing this? (more details in :thread: )\n```transport:\n  type: http\n  url: <https://my.fake-marquez-endpoint.com>\n  auth:\n    type: some.fully.qualified.classpath```","u":"U04AZ7992SU","t":"1694466446.599329"},{"m":"<!channel>\nThe first Toronto OpenLineage Meetup, featuring a presentation by recent adopter <https://metaphor.io/|Metaphor>, is just one week away. On the agenda:\n1. *Evolution of spec presentation/discussion (project background/history)*\n2. *State of the community*\n3. *Integrating OpenLineage with <https://metaphor.io/|Metaphor> (by special guests <https://www.linkedin.com/in/yeliu84/|Ye> &amp; <https://www.linkedin.com/in/ivanperepelitca/|Ivan>)*\n4. *Spark/Column lineage update*\n5. *Airflow Provider update*\n6. *Roadmap Discussion*\n*Find more details and RSVP <https://www.meetup.com/openlineage/events/295488014/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link|here>*.","u":"U02LXF3HUN7","t":"1694441261.486759"},{"m":"<!channel>\nThis month’s TSC meeting is next Thursday the 14th at 10am PT. On the tentative agenda:\n• announcements\n• recent releases\n• demo: Spark integration tests in Databricks runtime\n• open discussion\n• more (TBA)\nMore info and the meeting link can be found on the <https://openlineage.io/meetings/|website>. All are welcome! Also, feel free to reply or DM me with discussion topics, agenda items, etc.","u":"U02LXF3HUN7","t":"1694113940.400549"},{"m":"Has there been any conversation on the extensibility of facets/concepts? E.g.:\n• how does one extends the list of run states <https://openlineage.io/docs/spec/run-cycle> to add a paused/resumed state?\n• how does one extend <https://openlineage.io/docs/spec/facets/run-facets/nominal_time> to add a created at field?","u":"U0595Q78HUG","t":"1694036652.124299"},{"m":"Hello Everyone,\n\nI've been diving into the Marquez codebase and found a performance bottleneck in `JobDao.java`  for the query related to `namespaceName=MyNameSpace` `limit=10` and 12s with `limit=25`. I managed to optimize it using CTEs, and the execution times dropped dramatically to 300ms (for `limit=100`) and under 100ms (for `limit=25` ) on the same cluster.\nIssue link : <https://github.com/MarquezProject/marquez/issues/2608>\n\nI believe there's even more room for optimization, especially if we adjust the `job_facets_view` to include the `namespace_name` column.\n\nWould the team be open to a PR where I share the optimized query and discuss potential further refinements? I believe these changes could significantly enhance the Marquez web UI experience.\n\nPR link : <https://github.com/MarquezProject/marquez/pull/2609>\n\nLooking forward to your feedback.","u":"U05HBLE7YPL","t":"1694032987.624809"},{"m":"it looks like my dynamic task mapping in Airflow has the same run ID in marquez, so even if I am processing 100 files, there is only one version of the data. is there a way to have a separate version of each dynamic task so I can track the filename etc?","u":"U05NMJ0NBUK","t":"1693877705.781699"},{"m":"Also, another small clarification is that when using `MergeIntoCommand`, I'm receiving the lineage events on the backend, but I cannot seem to find any logging of the payload when I enable debug mode in openlineage. I remember there was a similar issue reported by another user in the past. May I check if it might be possible to help with this? It's making debugging quite hard for these cases. Thanks!","u":"U04EZ2LPDV4","t":"1693823945.734419"},{"m":"Hi guys, I'd like to capture the `spark.databricks.clusterUsageTags.clusterAllTags` property from databricks. However, the value of this is a list of keys, and therefore cannot be supported by custom environment facet builder.\nI was thinking that capturing this property might be useful for most databricks workloads, and whether it might make sense to auto-capture it along with other databricks variables, similar to how we capture mount points for the databricks jobs.\nDoes this sound okay? If so, then I can help to contribute this functionality","u":"U04EZ2LPDV4","t":"1693813108.356499"},{"m":"<!channel>\nThe <https://mailchi.mp/ba1d4031cbe0/openlineage-news-july-9586289?e=ef0563a7f8|latest issue of OpenLineage News> is out now! Please <http://bit.ly/OL_news|subscribe> to get it directly in your inbox each month.","u":"U02LXF3HUN7","t":"1693602981.025489"},{"m":"It sounds like there have been a few announcements at Google Next:\n<https://cloud.google.com/data-catalog/docs/how-to/open-lineage>\n<https://cloud.google.com/dataproc/docs/guides/lineage>","u":"U01DCLP0GU9","t":"1693519820.292119"},{"m":"Will the August meeting be put up at <https://wiki.lfaidata.foundation/display/OpenLineage/Monthly+TSC+meeting> soon? (usually it’s up in a  few days :slightly_smiling_face:","u":"U0323HG8C8H","t":"1693510399.153829"},{"m":"ok,it`s my first use thie lineage tool. first,I added dependences in my pom.xml like this:\n&lt;dependency&gt;\n            &lt;groupId&gt;io.openlineage&lt;/groupId&gt;\n            &lt;artifactId&gt;openlineage-java&lt;/artifactId&gt;\n            &lt;version&gt;0.12.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\n            &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;\n            &lt;version&gt;2.7&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\n            &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;\n            &lt;version&gt;2.7&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;\n            &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt;\n            &lt;version&gt;2.7&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.openlineage&lt;/groupId&gt;\n            &lt;artifactId&gt;openlineage-spark&lt;/artifactId&gt;\n            &lt;version&gt;0.30.1&lt;/version&gt;\n        &lt;/dependency&gt;\n\nmy spark version is 3.3.1 and the version can not change\n\nsecond, in file Openlineage/intergration/spark I enter command :  docker-compose up  and follow the steps in this doc:\n<https://openlineage.io/docs/integrations/spark/quickstart_local>\nthere is no erro when i use notebook to execute pyspark for openlineage and I could get json message.\nbut after I enter \"docker-compose up\" ,I want to use my Idea tool to execute scala code like above,the erro happend like above. It seems that I does not configure the  environment correctly. so how can i fix the problem .","u":"U05NGJ8AM8X","t":"1693468312.450209"},{"m":"hello,everyone,i can run openLineage spark code in my notebook with python,but when use my idea to execute scala code like this:\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.sql.SparkSession\nimport io.openlineage.client.OpenLineageClientUtils.loadOpenLineageYaml\nimport org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, SparkListenerApplicationStart}\nimport sun.java2d.marlin.MarlinUtils.logInfo\nobject Test {\n  def main(args: Array[String]): Unit = {\n\n    val spark = SparkSession\n      .builder()\n      .master(\"local\")\n      .appName(\"test\")\n      .config(\"spark.jars.packages\",\"io.openlineage:openlineage-spark:0.12.0\")\n      .config(\"spark.extraListeners\",\"io.openlineage.spark.agent.OpenLineageSparkListener\")\n      .config(\"spark.openlineage.transport.type\",\"console\")\n      .getOrCreate()\n\n    spark.sparkContext.setLogLevel(\"INFO\")\n\n    //spark.sparkContext.addSparkListener(new MySparkAppListener)\n    import spark.implicits._\n    val input = Seq((1, \"zs\", 2020), (2, \"ls\", 2023)).toDF(\"id\", \"name\", \"year\")\n\n    input.select(\"id\", \"name\").orderBy(\"id\").show()\n\n  }\n\n}\n\nthere is something wrong:\nException in thread \"spark-listener-group-shared\" java.lang.NoSuchMethodError: io.openlineage.client.OpenLineageClientUtils.loadOpenLineageYaml(Ljava/io/InputStream;)Lio/openlineage/client/OpenLineageYaml;\n\tat io.openlineage.spark.agent.ArgumentParser.extractOpenlineageConfFromSparkConf(ArgumentParser.java:114)\n\tat io.openlineage.spark.agent.ArgumentParser.parse(ArgumentParser.java:78)\n\tat io.openlineage.spark.agent.OpenLineageSparkListener.initializeContextFactoryIfNotInitialized(OpenLineageSparkListener.java:277)\n\tat io.openlineage.spark.agent.OpenLineageSparkListener.onApplicationStart(OpenLineageSparkListener.java:267)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:55)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat <http://org.apache.spark.scheduler.AsyncEventQueue.org|org.apache.spark.scheduler.AsyncEventQueue.org>$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n\ni want to know how can i set idea scala environment correctly","u":"U05NGJ8AM8X","t":"1693463508.522729"},{"m":"Can anyone let 3 people stuck downstairs into the 7th floor?","u":"U05EC8WB74N","t":"1693445911.744069"},{"m":"<!channel>\nFriendly reminder: there’s a meetup <https://openlineage.slack.com/archives/C01CK9T7HKR/p1692973763570629|tonight> at Astronomer’s offices in SF!","u":"U02LXF3HUN7","t":"1693410605.894959"},{"m":"Hi, Will really appreciate if someone can guide me or provide me any pointer -  if they have been able to implement authentication/authorization for access to Marquez. Have not seen much info around it. Any pointers greatly appreciated. Thanks in advance.","u":"U05JY6MN8MS","t":"1693397729.978309"},{"m":"for namespaces, if my data is moving between sources (SFTP -&gt; GCS -&gt; Azure Blob (synapse connects to parquet datasets) then should my namespace be based on the client I am working with? my current namespace has been <gcs://client-name> to refer to the bucket, but that falls apart when considering the data sources and some destinations. perhaps I should just add a field for client-name instead to have a consolidated view?","u":"U05NMJ0NBUK","t":"1693329152.193929"},{"m":"hi folks, for now I'm producing `.jsonl` (or `.ndjson` ) files with one event per line, do you know if there's any way to validate those? would standard JSON Schema tools work?","u":"U05HFGKEYVB","t":"1693300839.710459"},{"m":"Hello, I'm currently in the process of following the instructions outlined in the provided getting started guide at <https://openlineage.io/getting-started/>. However, I've encountered a problem while attempting to complete **Step 1** of the guide. Unfortunately, I'm encountering an internal server error at this stage. I did manage to successfully run Marquez, but it appears that there might be an issue that needs to be addressed. I have attached screen shots.","u":"U05QNRSQW1E","t":"1693293484.701439"},{"m":"New on the OpenLineage blog: <https://openlineage.io/blog/airflow-provider|a close look at the new OpenLineage Airflow Provider>, including:\n•  the critical improvements it brings to the integration\n• the high-level design\n• implementation details\n• an example operator\n• planned enhancements\n• a list of supported operators\n• more.\nThe post, by <@U01RA9B5GG2>, <@U01DCLP0GU9> and myself is live now on the OpenLineage blog.","u":"U02LXF3HUN7","t":"1693267537.810959"},{"m":"<!channel>\nThe agenda for the <https://www.meetup.com/openlineage/events/295488014/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link|Toronto Meetup at Airflow Summit> on 9/18 has been updated. This promises to be an exciting, richly productive discussion. Don’t miss it if you’ll be in the area!\n1. Intros\n2. Evolution of spec presentation/discussion (project background/history)\n3. State of the community\n4. Spark/Column lineage update\n5. Airflow Provider update \n6. Roadmap Discussion\n7. Action items review/next steps\n","u":"U02LXF3HUN7","t":"1693258111.112809"},{"m":"Hi folks. I have some pure golang jobs from which I need to emit OL events to Marquez.  Is the right way to go about this to generate a Golang client from the Marquez OpenAPI spec and use that client from my go jobs?","u":"U05PVS8GRJ6","t":"1693243558.640159"},{"m":"and something else: I understand that Marquez does not yet support the 2.0 spec, hence it's incompatible with static metadata right? I tried to emit a list of `DatasetEvent` s and got `HTTPError: 422 Client Error: Unprocessable Entity for url: <http://localhost:3000/api/v1/lineage>` (I'm using a `FileTransport` for now)","u":"U05HFGKEYVB","t":"1693212561.369509"},{"m":"hi folks, I'm looking into exporting static metadata, and found that `DatasetEvent` requires a `eventTime`, which in my mind doesn't make sense for static events. I'm setting it to `None` and the Python client seems to work, but wanted to ask if I'm missing something.","u":"U05HFGKEYVB","t":"1693212473.810659"},{"m":"hi Openlineage team , we would like to join one of your meetups(me and <@U05HK41VCH1> nad @Phil Rolph and we're wondering if you are hosting any meetups after the 18/9 ? We are trying to join this but air - tickets are quite expensive","u":"U05J5GRKY10","t":"1692975450.380969"},{"m":"<!channel>\nFriendly reminder: our next in-person meetup is next Wednesday, August 30th in San Francisco at Astronomer’s offices in the Financial District. You can sign up and find the details on the <https://www.meetup.com/meetup-group-bnfqymxe/events/295195280/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link|meetup event page>.","u":"U02LXF3HUN7","t":"1692973763.570629"},{"m":"<!channel>\nWe released OpenLineage 1.1.0, including:\nAdditions:\n• Flink: create Openlineage configuration based on Flink configuration `#2033` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\n• Java: add Javadocs to the Java client `#2004` <https://github.com/julienledem|@julienledem>\n• Spark: append output dataset name to a job name `#2036` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\n• Spark: support Spark 3.4.1 `#2057` <https://github.com/pawel-big-lebowski|@pawel-big-lebowski>\nFixes:\n• Flink: fix a bug when getting schema for `KafkaSink` `#2042` <https://github.com/pentium3|@pentium3>\n• Spark: fix ignored event `adaptive_spark_plan` in Databricks `#2061` <https://github.com/algorithmy1|@algorithmy1>\nPlus additional bug fixes, doc changes and more.\nThanks to all the contributors, especially new contributors @pentium3 and <@U05HBLE7YPL>!\n*Release:* <https://github.com/OpenLineage/OpenLineage/releases/tag/1.1.0>\n*Changelog:* <https://github.com/OpenLineage/OpenLineage/blob/main/CHANGELOG.md>\n*Commit history:* <https://github.com/OpenLineage/OpenLineage/compare/1.0.0...1.1.0>\n*Maven:* <https://oss.sonatype.org/#nexus-search;quick~openlineage>\n*PyPI:* <https://pypi.org/project/openlineage-python/>","u":"U02LXF3HUN7","t":"1692817450.338859"},{"m":"Hey folks! Do we have clear step-by-step documentation on how we can leverage the `ServiceLoader`  based approach for injecting specific OpenLineage customisations for tweaking the transport type with defaults / tweaking column level lineage etc?","u":"U05JBHLPY8K","t":"1692810528.463669"},{"m":"Approve a new release please :slightly_smiling_face:\n• Fix spark integration filtering Databricks events. ","u":"U05HBLE7YPL","t":"1692802510.386629"}],"C01NAFMBVEY":[],"C030F1J0264":[],"C04E3Q18RR9":[],"C04JPTTC876":[],"C04QSV0GG23":[],"C04THH1V90X":[],"C051C93UZK9":[],"C055GGUFMHQ":[],"C056YHEU680":[{"m":"<@U05TQPZ4R4L> has joined the channel","u":"U05TQPZ4R4L","t":"1695502057.851259"},{"m":"Some pictures from last night","u":"U01DCLP0GU9","t":"1693538290.446329"},{"m":"<@U05Q3HT6PBR> has joined the channel","u":"U05Q3HT6PBR","t":"1693520941.967439"},{"m":"Time: 5:30-8:30 pm\nAddress: 8 California St., San Francisco, CA, seventh floor\nGetting in: someone from Astronomer will be in the lobby to direct you","u":"U02LXF3HUN7","t":"1693422775.587509"},{"m":"Adding the venue info in case it’s more convenient than the meetup page:","u":"U02LXF3HUN7","t":"1693422678.406409"}],"C05N442RQUA":[{"m":"Hi, if you’re wondering if you’re in the right place: look for Uncle Tetsu’s Cheesecake nextdoor and for the address (600 Bay St) above the door. The building is an older one (unlike the meeting space itself, which is modern and well-appointed)","u":"U02LXF3HUN7","t":"1695068433.208409"},{"m":"Looking forward to seeing you on Monday! Here’s the time/place info again for your convenience:\n• Date: 9/18\n• Time: 5-8:00 PM ET\n• Place: Canarts, 600 Bay St., #410 (around the corner from the Airflow Summit venue)\n• Venue phone: <tel:4168052286|416-805-2286>\n• Meetup page with more info and signup: <https://www.meetup.com/openlineage/events/295488014/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link>\nPlease send a message if you find yourself stuck in the lobby, etc.","u":"U02LXF3HUN7","t":"1694794649.751439"},{"m":"<@U05SXDWVA7K> has joined the channel","u":"U05SXDWVA7K","t":"1694787071.646859"},{"m":"<!channel>\nIt’s hard to believe this is happening in just one week! Here’s the updated agenda:\n1. *Intros*\n2. *Evolution of spec presentation/discussion (project background/history)*\n3. *State of the community*\n4. *Integrating OpenLineage with <https://metaphor.io/|Metaphor> (by special guests <https://www.linkedin.com/in/yeliu84/|Ye> &amp; <https://www.linkedin.com/in/ivanperepelitca/|Ivan>)*\n5. *Spark/Column lineage update*\n6. *Airflow Provider update*\n7. *Roadmap Discussion*\n8. *Action items review/next steps*\nFind the details and RSVP <https://www.meetup.com/openlineage/events/295488014/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link|here>.","u":"U02LXF3HUN7","t":"1694441637.116609"},{"m":"Most OpenLineage regular contributors will be there. It will be fun to be all in person. Everyone is encouraged to join","u":"U01DCLP0GU9","t":"1693624251.155569"},{"m":"really looking forward to meeting all of you in Toronto!!","u":"U01HNKK4XAM","t":"1692984822.264569"},{"m":"Some belated updates on this in case you’re not aware:\n• Date: 9/18\n• Time: 5-8:00 PM ET\n• Place: Canarts, 600 Bay St., #410 (around the corner from the Airflow Summit venue)\n• Venue phone: <tel:4168052286|416-805-2286>\n• Meetup for more info and to sign up: <https://www.meetup.com/openlineage/events/295488014/?utm_medium=referral&amp;utm_campaign=share-btn_savedevents_share_modal&amp;utm_source=link>","u":"U02LXF3HUN7","t":"1692984607.290939"}],"C05PD7VJ52S":[{"m":"<@U05QHG1NJ8J> has joined the channel","u":"U05QHG1NJ8J","t":"1693512257.995209"},{"m":"<@U01RA9B5GG2> has joined the channel","u":"U01RA9B5GG2","t":"1692984861.392349"},{"m":"Yes, hope so! Thank you for your interest in joining a meetup!","u":"U02LXF3HUN7","t":"1692983998.092189"},{"m":"hopefully meet you soon in London","u":"U05HK41VCH1","t":"1692982281.255129"},{"m":"Thanks Michael for starting this channel","u":"U05HK41VCH1","t":"1692982268.668049"},{"m":"yes absolutely will give you an answer by Monday","u":"U05J5GRKY10","t":"1692979169.558489"},{"m":"OK! Would you please let me know when you know, and we’ll go from there?","u":"U02LXF3HUN7","t":"1692979126.425069"},{"m":"and if that not the case i can provide personal space","u":"U05J5GRKY10","t":"1692979080.273399"},{"m":"I am pretty sure you can use our 6point6 offices or at least part of it","u":"U05J5GRKY10","t":"1692979067.390839"},{"m":"I will have to confirm but 99% yes","u":"U05J5GRKY10","t":"1692978958.692039"},{"m":"Great! Do you happen to have space we could use?","u":"U02LXF3HUN7","t":"1692978871.220909"},{"m":"thats perfect !","u":"U05J5GRKY10","t":"1692978852.583609"},{"m":"Hi George, nice to meet you. Thanks for asking about future meetups. Would November be too soon, or what’s a good timeframe for you all?","u":"U02LXF3HUN7","t":"1692978834.508549"},{"m":"thanks so much !","u":"U05J5GRKY10","t":"1692978816.416879"},{"m":"Hi Michael","u":"U05J5GRKY10","t":"1692978769.711289"},{"m":"<@U05HK41VCH1> has joined the channel","u":"U05HK41VCH1","t":"1692978760.556819"},{"m":"<@U01HNKK4XAM> has joined the channel","u":"U01HNKK4XAM","t":"1692978735.461499"},{"m":"<@U05J5GRKY10> has joined the channel","u":"U05J5GRKY10","t":"1692978735.367849"},{"m":"<@U02LXF3HUN7> has joined the channel","u":"U02LXF3HUN7","t":"1692978725.076179"}],"C05U3UC85LM":[{"m":"<@U0620HU51HA> has joined the channel","u":"U0620HU51HA","t":"1697734944.171989"},{"m":"<@U05U9K21LSG> would be great if we could get your eyes on this PR: <https://github.com/OpenLineage/OpenLineage/pull/2134>","u":"U01HNKK4XAM","t":"1697201275.793679"},{"m":"Just seeing this, we had a company holiday yesterday. Yes, fluent data sources are our new way of connecting to data and the older \"block-style\" is deprecated and will be removed when we cut 0.18.0. I'm not sure of the timing of that but likely in the next couple months.","u":"U05U9K21LSG","t":"1696979522.397239"},{"m":"<@U05U9929K3N> <@U05U9K21LSG> ^^","u":"U01HNKK4XAM","t":"1696860879.713919"},{"m":"Hello guys! I’ve been looking recently into changes in GX.\n<https://greatexpectations.io/blog/the-fluent-way-to-connect-to-data-sources-in-gx/>\nis this the major change you’d like to introduce in OL&lt;-&gt; GX?","u":"U02S6F54MAB","t":"1696852012.119669"},{"m":"<@U05U9K21LSG> has joined the channel","u":"U05U9K21LSG","t":"1695916771.601439"},{"m":"<@U05U9929K3N> it was great meeting earlier, looking forward to collaborating on this!","u":"U01HNKK4XAM","t":"1695840258.594299"},{"m":"<@U01RA9B5GG2> has joined the channel","u":"U01RA9B5GG2","t":"1695836477.653589"},{"m":"<@U02S6F54MAB> has joined the channel","u":"U02S6F54MAB","t":"1695836303.953919"},{"m":"<@U01HNKK4XAM> has joined the channel","u":"U01HNKK4XAM","t":"1695836303.827639"},{"m":"<@U05U9929K3N> has joined the channel","u":"U05U9929K3N","t":"1695836303.727049"},{"m":"<@U02LXF3HUN7> has joined the channel","u":"U02LXF3HUN7","t":"1695836283.617309"}],"C065PQ4TL8K":[{"m":"Maybe move today's meeting earlier, since no one from west coast is joining? <@U01HNKK4XAM>","u":"U01RA9B5GG2","t":"1700562211.366219"},{"m":"I’m off on vacation. See you in a week","u":"U01DCLP0GU9","t":"1700272614.735719"},{"m":"just searching for OpenLineage in the Datahub code base. They have an “interesting” approach? <https://github.com/datahub-project/datahub/blob/2b0811b9875d7d7ea11fb01d0157a21fdd67f020/metadata-ingestion-modules/airflow-plugin/src/datahub_airflow_plugin/_extractors.py#L14|https://github.com/datahub-project/datahub/blob/2b0811b9875d7d7ea11fb01d0157a21fdd[…]odules/airflow-plugin/src/datahub_airflow_plugin/_extractors.py>","u":"U01DCLP0GU9","t":"1700246539.228259"},{"m":"CFP for Berlin Buzzwords went up: <https://2024.berlinbuzzwords.de/call-for-papers/>\nStill over 3 months to submit :slightly_smiling_face:","u":"U01RA9B5GG2","t":"1700155042.082759"},{"m":"worlds are colliding: 6point6 has been acquired by Accenture","u":"U02LXF3HUN7","t":"1700145084.414099"},{"m":"Any opinions about a free task management alternative to the free version of Notion (10-person limit)? Looking at Trello for keeping track of talks.","u":"U02LXF3HUN7","t":"1700088623.669029"},{"m":"have we discussed adding column level lineage support to Airflow? <https://marquezproject.slack.com/archives/C01E8MQGJP7/p1700087438599279?thread_ts=1700084629.245949&amp;cid=C01E8MQGJP7>","u":"U01DCMDFHBK","t":"1700087546.032789"},{"m":"Apparently an admin can view a Slack archive at any time at this URL: <https://openlineage.slack.com/services/export|https://openlineage.slack.com/services/export>. Only public channels are available, though.","u":"U02LXF3HUN7","t":"1700078359.877599"},{"m":"Anyone have thoughts about how to address the question about “pain points” here? <https://openlineage.slack.com/archives/C01CK9T7HKR/p1700064564825909|https://openlineage.slack.com/archives/C01CK9T7HKR/p1700064564825909>. (Listing pros is easy — it’s the cons we don’t have boilerplate for)","u":"U02LXF3HUN7","t":"1700078230.775579"},{"m":"is it time to *support hudi*?","u":"U02S6F54MAB","t":"1700068651.517579"},{"m":"Got the doc + poc for hook-level coverage: <https://docs.google.com/document/d/1q0shiUxopASO8glgMqjDn89xigJnGrQuBMbcRdolUdk/edit?usp=sharing>","u":"U01RA9B5GG2","t":"1700066684.350639"},{"m":"hey look, more fun\n<https://github.com/OpenLineage/OpenLineage/pull/2263>","u":"U02S6F54MAB","t":"1700040937.040239"},{"m":"also, what about this PR? <https://github.com/MarquezProject/marquez/pull/2654>","u":"U01DCMDFHBK","t":"1700037370.235629"},{"m":"`_Minor_: We can consider defining a _run_state column and eventually dropping the event_type. That is, we can consider columns prefixed with _ to be \"remappings\" of OL properties to Marquez.` -&gt; didn't get this one. Is it for now or some future plans?","u":"U02MK6YNAQ5","t":"1700037282.474539"},{"m":"<@U02MK6YNAQ5> approved PR <https://github.com/MarquezProject/marquez/pull/2661|#2661> with minor comments, I think the <https://github.com/MarquezProject/marquez/pull/2661/files#r1393820409|enum defined in the db layer> is one comment we’ll need to address before merging; otherwise solid work dude :ok_hand:","u":"U01DCMDFHBK","t":"1700037147.106879"},{"m":"<https://github.com/OpenLineage/OpenLineage/pull/2260>\nfun PR incoming","u":"U02S6F54MAB","t":"1700004648.584649"},{"m":":wave:","u":"U01RA9B5GG2","t":"1699987988.642529"},{"m":":ocean:","u":"U053LLVTHRN","t":"1699982987.125549"},{"m":":wave:","u":"U01DCMDFHBK","t":"1699982333.485469"},{"m":":wave:  ","u":"U01DCLP0GU9","t":"1699982322.990799"},{"m":":wave:","u":"U02LXF3HUN7","t":"1699982179.651129"},{"m":"<@U053LLVTHRN> has joined the channel","u":"U053LLVTHRN","t":"1699982042.414079"},{"m":":wave:","u":"U02S6F54MAB","t":"1699982037.267109"},{"m":"<@U05KKM07PJP> has joined the channel","u":"U05KKM07PJP","t":"1699982026.646699"},{"m":"<@U01DCMDFHBK> has joined the channel","u":"U01DCMDFHBK","t":"1699982026.554329"},{"m":"<@U02LXF3HUN7> has joined the channel","u":"U02LXF3HUN7","t":"1699982026.462039"},{"m":"<@U02S6F54MAB> has joined the channel","u":"U02S6F54MAB","t":"1699982026.350149"},{"m":"<@U02MK6YNAQ5> has joined the channel","u":"U02MK6YNAQ5","t":"1699982026.266479"},{"m":"<@U01DCLP0GU9> has joined the channel","u":"U01DCLP0GU9","t":"1699982026.191829"},{"m":"<@U01RA9B5GG2> has joined the channel","u":"U01RA9B5GG2","t":"1699981990.850589"},{"m":"<@U01HNKK4XAM> has joined the channel","u":"U01HNKK4XAM","t":"1699981986.459199"}]},"pages":{"C01CK9T7HKR":["1692802510.386629","1692802510.386629"],"C056YHEU680":["1693422678.406409","1693422678.406409"],"C05N442RQUA":["1692984607.290939","1692984607.290939"],"C05PD7VJ52S":["1692978725.076179","1692978725.076179"],"C05U3UC85LM":["1695836283.617309","1695836283.617309"],"C065PQ4TL8K":["1699981986.459199","1699981986.459199"]}};